Traceback (most recent call last):
  File "/Users/knf792/miniconda3/envs/nlp-course/lib/python3.10/site-packages/jupyter_cache/executors/utils.py", line 51, in single_nb_execution
    executenb(
  File "/Users/knf792/miniconda3/envs/nlp-course/lib/python3.10/site-packages/nbclient/client.py", line 1204, in execute
    return NotebookClient(nb=nb, resources=resources, km=km, **kwargs).execute()
  File "/Users/knf792/miniconda3/envs/nlp-course/lib/python3.10/site-packages/nbclient/util.py", line 84, in wrapped
    return just_run(coro(*args, **kwargs))
  File "/Users/knf792/miniconda3/envs/nlp-course/lib/python3.10/site-packages/nbclient/util.py", line 62, in just_run
    return loop.run_until_complete(coro)
  File "/Users/knf792/miniconda3/envs/nlp-course/lib/python3.10/asyncio/base_events.py", line 646, in run_until_complete
    return future.result()
  File "/Users/knf792/miniconda3/envs/nlp-course/lib/python3.10/site-packages/nbclient/client.py", line 663, in async_execute
    await self.async_execute_cell(
  File "/Users/knf792/miniconda3/envs/nlp-course/lib/python3.10/site-packages/nbclient/client.py", line 965, in async_execute_cell
    await self._check_raise_for_error(cell, cell_index, exec_reply)
  File "/Users/knf792/miniconda3/envs/nlp-course/lib/python3.10/site-packages/nbclient/client.py", line 862, in _check_raise_for_error
    raise CellExecutionError.from_cell_and_msg(cell, exec_reply_content)
nbclient.exceptions.CellExecutionError: An error occurred while executing the following cell:
------------------
import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()
import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline

with tf.Graph().as_default():
    ## Placeholders
    # [batch_size x max_premise_length]
    premises_pl = tf.placeholder(tf.int64, [None, None], "premises")
    # [batch_size]
    premise_lengths_pl = tf.placeholder(tf.int64, [None], "premise_lengths")
    # [batch_size x max_hypothesis_length]
    hypotheses_pl = tf.placeholder(tf.int64, [None, None], "hypotheses")
    # [batch_size]
    hypothesis_lengths_pl = tf.placeholder(tf.int64, [None], "hypothesis_lengths")
    # [batch_size]
    labels_pl = tf.placeholder(tf.int64, [None], "labels")

    ## Model
    input_size = 2
    hidden_size = 5
    target_size = 3
    vocab_size = len(vocab)

    embeddings = tf.get_variable("W", [vocab_size, input_size])

    # [batch_size x max_premise_length x input_size]
    premises_embedded = tf.nn.embedding_lookup(embeddings, premises_pl)
    # [batch_size x max_hypothesis_length x input_size]
    hypotheses_embedded = tf.nn.embedding_lookup(embeddings, hypotheses_pl)

    with tf.variable_scope("encoder") as varscope:
        cell = tf.nn.rnn_cell.LSTMCell(hidden_size, state_is_tuple=True)
        _, premise_final_state = \
            tf.nn.dynamic_rnn(cell, premises_embedded, sequence_length=premise_lengths_pl, dtype=tf.float32)    
        # [batch_size x hidden_size]
        premises_h = premise_final_state.h
        varscope.reuse_variables()  # using the same encoder for premises and hypotheses
        _, hypothesis_final_state = \
            tf.nn.dynamic_rnn(cell, hypotheses_embedded, sequence_length=hypothesis_lengths_pl, dtype=tf.float32)  
        # [batch_size x hidden_size]
        hypotheses_h = hypothesis_final_state.h
         
    # [batch_size x 2*hidden_size]
    pair_representation = tf.concat([premises_h, hypotheses_h], 1)
        
    # [batch_size x target_size]
    logits = tf.layers.dense(pair_representation, target_size)
        
    probability = tf.nn.softmax(logits)
    
    ## Training Loss
    loss = tf.reduce_sum(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels_pl))
            
    ## Optimizer
    optim = tf.train.AdamOptimizer(0.1)
    optim_op = optim.minimize(loss)
        
    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())
        
        feed_dict = {
            premises_pl: premises_np,
            premise_lengths_pl: premise_lengths,
            hypotheses_pl: hypotheses_np,
            hypothesis_lengths_pl: hypothesis_lengths,
            labels_pl: labels
        }
        
        for i in range(10):
            _, current_loss, current_probabilities = sess.run([optim_op, loss, probability], feed_dict)
            print("Epoch:", i, "Loss:", current_loss)
            sns.heatmap(current_probabilities, vmin=0.0, vmax=1.0, square=True, cmap="Blues") 
            plt.show()
------------------

[0;31m---------------------------------------------------------------------------[0m
[0;31mModuleNotFoundError[0m                       Traceback (most recent call last)
Input [0;32mIn [2][0m, in [0;36m<cell line: 1>[0;34m()[0m
[0;32m----> 1[0m [38;5;28;01mimport[39;00m [38;5;21;01mtensorflow[39;00m[38;5;21;01m.[39;00m[38;5;21;01mcompat[39;00m[38;5;21;01m.[39;00m[38;5;21;01mv1[39;00m [38;5;28;01mas[39;00m [38;5;21;01mtf[39;00m
[1;32m      2[0m tf[38;5;241m.[39mdisable_v2_behavior()
[1;32m      3[0m [38;5;28;01mimport[39;00m [38;5;21;01mseaborn[39;00m [38;5;28;01mas[39;00m [38;5;21;01msns[39;00m

[0;31mModuleNotFoundError[0m: No module named 'tensorflow'
ModuleNotFoundError: No module named 'tensorflow'

