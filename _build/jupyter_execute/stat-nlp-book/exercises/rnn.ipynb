{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "# Exercise: Recognizing Textual Entailment\n",
    "\n",
    "In this exercise, we are going to implement some of the models discussed in the Deep Learning for Natural Language Processing chapter. Specifically, we are going to implement an RTE system using TensorFlow. Instead of running this on a large corpus like SNLI, we are working on a very small corpus for implementation purposes. This is a common practice as in early stages of development we will likely encounter bugs in our cope and compile and run-time errors for which we do not need to train on a lot of data. It is generally a good idea to first test whether our model can overfit a tiny debug corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-16T11:48:52.328673",
     "start_time": "2016-12-16T11:48:52.300413"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# examples from SNLI training corpus, 2=entailment, 1=neutral, 0=contradiction\n",
    "# premise, hypothesis, label\n",
    "data = [\n",
    "    (\"Children smiling and waving at camera\", \"They are smiling at their parents\", 1),\n",
    "    (\"A boy is jumping on skateboard in the middle of a red bridge.\", \"The boy does a skateboarding trick.\", 2),\n",
    "    (\"A boy is jumping on skateboard in the middle of a red bridge.\", \"The boy skates down the sidewalk.\", 0),\n",
    "    (\"A person on a horse jumps over a broken down airplane.\", \"A person is outdoors, on a horse.\", 2),    \n",
    "    (\"A woman in a green jacket and hood over her head looking towards a valley.\", \"The woman is cold.\", 1),\n",
    "    (\"A couple playing with a little boy on the beach.\", \"A couple watch a little girl play by herself on the beach.\", 0)\n",
    "]\n",
    "\n",
    "def data2np(data, PAD=0):\n",
    "    \"\"\"Transforms data into a list of numpy tensors.\"\"\"\n",
    "    premises = []; premise_lengths = []\n",
    "    hypotheses = []; hypothesis_lengths = []\n",
    "    labels = []    \n",
    "    for premise, hypothesis, label in data:\n",
    "        premise_tokenized = premise.split(\" \")\n",
    "        premises.append(premise_tokenized)\n",
    "        premise_lengths.append(len(premise_tokenized))\n",
    "        hypotheses_tokenized = hypothesis.split(\" \")\n",
    "        hypotheses.append(hypotheses_tokenized)\n",
    "        hypothesis_lengths.append(len(hypotheses_tokenized))\n",
    "        labels.append(label)\n",
    "    vocab = {\"<PAD>\": PAD}\n",
    "    premises_np = np.full([len(data), np.max(premise_lengths)], PAD)\n",
    "    hypotheses_np = np.full([len(data), np.max(hypothesis_lengths)], PAD)    \n",
    "    for k, seqs in enumerate([premises, hypotheses]):\n",
    "        for i, seq in enumerate(seqs):\n",
    "            for j, word in enumerate(seq):\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = len(vocab)\n",
    "                seq[j] = vocab[word] \n",
    "            if k == 0:\n",
    "                premises_np[i, 0:premise_lengths[i]] = seq\n",
    "            else:\n",
    "                hypotheses_np[i, 0:hypothesis_lengths[i]] = seq\n",
    "    return premises_np, premise_lengths, hypotheses_np, hypothesis_lengths, labels, vocab\n",
    "\n",
    "premises_np, premise_lengths, hypotheses_np, hypothesis_lengths, labels, vocab = data2np(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-16T11:48:57.272579",
     "start_time": "2016-12-16T11:48:53.018519"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      2\u001b[0m tf\u001b[38;5;241m.\u001b[39mdisable_v2_behavior()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    ## Placeholders\n",
    "    # [batch_size x max_premise_length]\n",
    "    premises_pl = tf.placeholder(tf.int64, [None, None], \"premises\")\n",
    "    # [batch_size]\n",
    "    premise_lengths_pl = tf.placeholder(tf.int64, [None], \"premise_lengths\")\n",
    "    # [batch_size x max_hypothesis_length]\n",
    "    hypotheses_pl = tf.placeholder(tf.int64, [None, None], \"hypotheses\")\n",
    "    # [batch_size]\n",
    "    hypothesis_lengths_pl = tf.placeholder(tf.int64, [None], \"hypothesis_lengths\")\n",
    "    # [batch_size]\n",
    "    labels_pl = tf.placeholder(tf.int64, [None], \"labels\")\n",
    "\n",
    "    ## Model\n",
    "    input_size = 2\n",
    "    hidden_size = 5\n",
    "    target_size = 3\n",
    "    vocab_size = len(vocab)\n",
    "\n",
    "    embeddings = tf.get_variable(\"W\", [vocab_size, input_size])\n",
    "\n",
    "    # [batch_size x max_premise_length x input_size]\n",
    "    premises_embedded = tf.nn.embedding_lookup(embeddings, premises_pl)\n",
    "    # [batch_size x max_hypothesis_length x input_size]\n",
    "    hypotheses_embedded = tf.nn.embedding_lookup(embeddings, hypotheses_pl)\n",
    "\n",
    "    with tf.variable_scope(\"encoder\") as varscope:\n",
    "        cell = tf.nn.rnn_cell.LSTMCell(hidden_size, state_is_tuple=True)\n",
    "        _, premise_final_state = \\\n",
    "            tf.nn.dynamic_rnn(cell, premises_embedded, sequence_length=premise_lengths_pl, dtype=tf.float32)    \n",
    "        # [batch_size x hidden_size]\n",
    "        premises_h = premise_final_state.h\n",
    "        varscope.reuse_variables()  # using the same encoder for premises and hypotheses\n",
    "        _, hypothesis_final_state = \\\n",
    "            tf.nn.dynamic_rnn(cell, hypotheses_embedded, sequence_length=hypothesis_lengths_pl, dtype=tf.float32)  \n",
    "        # [batch_size x hidden_size]\n",
    "        hypotheses_h = hypothesis_final_state.h\n",
    "         \n",
    "    # [batch_size x 2*hidden_size]\n",
    "    pair_representation = tf.concat([premises_h, hypotheses_h], 1)\n",
    "        \n",
    "    # [batch_size x target_size]\n",
    "    logits = tf.layers.dense(pair_representation, target_size)\n",
    "        \n",
    "    probability = tf.nn.softmax(logits)\n",
    "    \n",
    "    ## Training Loss\n",
    "    loss = tf.reduce_sum(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels_pl))\n",
    "            \n",
    "    ## Optimizer\n",
    "    optim = tf.train.AdamOptimizer(0.1)\n",
    "    optim_op = optim.minimize(loss)\n",
    "        \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        feed_dict = {\n",
    "            premises_pl: premises_np,\n",
    "            premise_lengths_pl: premise_lengths,\n",
    "            hypotheses_pl: hypotheses_np,\n",
    "            hypothesis_lengths_pl: hypothesis_lengths,\n",
    "            labels_pl: labels\n",
    "        }\n",
    "        \n",
    "        for i in range(10):\n",
    "            _, current_loss, current_probabilities = sess.run([optim_op, loss, probability], feed_dict)\n",
    "            print(\"Epoch:\", i, \"Loss:\", current_loss)\n",
    "            sns.heatmap(current_probabilities, vmin=0.0, vmax=1.0, square=True, cmap=\"Blues\") \n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task\n",
    "Extend the above model, for instance, by:\n",
    "  - Multi-layer instead of single-layer perceptron for prediction\n",
    "  - Bucketing\n",
    "  - Batching (instead of passing the entire dataset to the model)\n",
    "    - Make sure to draw random batches from the data\n",
    "  - Stacked RNNs (see `tf.nn.rnn_cell.MultiRNNCell`)\n",
    "  - Dropout on the input and output embeddings (see `tf.nn.dropout`)\n",
    "  - L2 regularization (see `tf.nn.l2_loss`)\n",
    "  - Gradient clipping (see `tf.clip_by_value` or `tf.clip_by_norm`)\n",
    "  - Conditional encoding\n",
    "  - Bi-directional RNNs\n",
    "  - Attention\n",
    "  - Word-by-word attention\n",
    "  - Larger portion of SNLI corpus (see http://nlp.stanford.edu/projects/snli/)\n",
    "  - Pre-trained word representations\n",
    "  - Better tokenization\n",
    "  - Early stopping\n",
    "  - Hyper-parameter optimization (e.g. random search)\n",
    "    - Initial learning rate\n",
    "    - Dropout probability\n",
    "    - Input and output size\n",
    "    - L2 regularization\n",
    "    - Gradient clipping value\n",
    "    - Batch size\n",
    "    - ...\n",
    "  - Any ideas from papers on nlp.stanford.edu/projects/snli/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}